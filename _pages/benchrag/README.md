# BenchRAG: A Modular RAG Evaluation Toolkit
A modular and extensible Retrieval-Augmented Generation (RAG) evaluation framework, including independent modules for query interpretation, retrieval, compression, and answer generation.

This project separates the RAG pipeline into four independent, reusable components:
- **Interpreter**: Understands query intent, expands or decomposes complex questions
- **Retriever**: Fetches relevant documents from a corpus
- **Compressor**: Compresses context using extractive or generative methods
- **Generator**: Generates answers based on the compressed context

---

## ğŸ§± Project Structure

```text
BenchRAG/
â”œâ”€â”€ interpreter/ # Query understanding and expansion
â”œâ”€â”€ retriever/ # BM25, dense, hybrid retrievers
â”œâ”€â”€ compressor/ # LLM or rule-based compressors
â”œâ”€â”€ generator/ # LLM-based answer generators
â”œâ”€â”€ datasets/ # Loaders for BEIR, MTEB, HotpotQA, Bright
â”œâ”€â”€ pipelines/ # Full RAG pipeline runner
â”œâ”€â”€ examples/ # examples for running each component
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```


---

## âš™ï¸ Installation

```bash
git clone https://github.com/gomate-community/BenchRAG.git
cd BenchRAG
pip install -r requirements.txt
```